{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "222ac98b-17f8-4f74-b3f4-5c6bb5b1ac94",
   "metadata": {},
   "source": [
    "# Reinforcement learning project\n",
    "\n",
    "\n",
    "This is a reinforcement learning project on deep Q-learning and policy gradient methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31248767-7375-4227-adf4-711fc4b7a900",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn as nn\n",
    "from torch import optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275bd312-8bc6-4576-a485-b9ddf885df84",
   "metadata": {},
   "source": [
    "## 1) Lunar Lander environment\n",
    "\n",
    "Your objective is to understand how the Lunar Lander environment works and what is the problem we want to solve.\n",
    "You need to write what are the states, the rewards and the actions.\n",
    "Write down the Markov decision process associated to the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "849a743a-175b-4446-8f32-60846c55ff90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_reward = -235.50002544373228\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Run a random policy.\"\"\"\n",
    "\n",
    "# Create and reset environment\n",
    "env = gym.make(\"LunarLander-v3\", continuous=False, render_mode=\"rgb_array\")\n",
    "obsevation, info = env.reset(seed=None)\n",
    "total_reward = 0.0\n",
    "\n",
    "# While the episode is not finished\n",
    "finished = False\n",
    "while not finished:\n",
    "\n",
    "    # Select a random action\n",
    "    action = env.action_space.sample()\n",
    "\n",
    "    # One step forward\n",
    "    obsevation, reward, terminated, truncated, info = env.step(action)\n",
    "    finished = terminated or truncated\n",
    "\n",
    "    # Eventually render the environment (render mode should be \"human\")\n",
    "    total_reward += reward\n",
    "    env.render()\n",
    "\n",
    "# Print reward\n",
    "print(\"total_reward = {}\".format(total_reward))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d39fca-a26f-4e1f-9f58-25726173ac36",
   "metadata": {},
   "source": [
    "## 2) Deep neural Q-network\n",
    "\n",
    "We aim to build a deep Q-network $ Q_\\theta(s, a) $, which estimates the Q-value for each action $ a $ given a state $ s $.\n",
    "This network is parameterized by weights $ \\theta $ and replaces the classical Q-table used in tabular methods.\n",
    "\n",
    "During learning, the network is updated to minimize the temporal difference (TD) error: $\\delta = r + \\gamma \\max_{a'} Q_\\theta(s', a') - Q_\\theta(s, a)$.\n",
    "This leads to the Q-learning update rule: $Q_\\theta(s, a) \\leftarrow Q_\\theta(s, a) + \\alpha \\, \\delta$.\n",
    "In practice, we minimize the squared TD error using gradient descent.\n",
    "\n",
    "Below are 3 code samples.\n",
    "- A class implementing the Q-network. You must specify `input_size` and `nb_actions`.\n",
    "- A script to train the Q-network. Complete the missing parts of the code.\n",
    "- A script to test the Q-network. Determine how to select an action from the Q-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b271d32a-fb72-4f64-8690-c463771be3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \"\"\"Deep neural Q-network.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize.\"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        # --> TODO: put the correct input and output sizes\n",
    "        input_size = 8\n",
    "        nb_actions = 4\n",
    "\n",
    "        # Layers\n",
    "        self.layer_a = nn.Linear(input_size, 128)\n",
    "        self.layer_b = nn.Linear(128, 128)\n",
    "        self.layer_c = nn.Linear(128, nb_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward.\"\"\"\n",
    "        x = F.relu(self.layer_a(x))\n",
    "        x = F.relu(self.layer_b(x))\n",
    "        q_values = self.layer_c(x)\n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "be960e66-3942-4671-b45f-4dc5139c51c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 5 - last reward: -375.65\n",
      "iteration 10 - last reward: -197.60\n",
      "iteration 15 - last reward: -162.55\n",
      "iteration 20 - last reward: -71.14\n",
      "iteration 25 - last reward: -165.36\n",
      "iteration 30 - last reward: -395.71\n",
      "iteration 35 - last reward: -98.30\n",
      "iteration 40 - last reward: -53.32\n",
      "iteration 45 - last reward: -182.16\n",
      "iteration 50 - last reward: -79.38\n",
      "iteration 55 - last reward: -85.73\n",
      "iteration 60 - last reward: -96.40\n",
      "iteration 65 - last reward: -141.19\n",
      "iteration 70 - last reward: -65.75\n",
      "iteration 75 - last reward: -86.09\n",
      "iteration 80 - last reward: -200.82\n",
      "iteration 85 - last reward: -235.93\n",
      "iteration 90 - last reward: -319.33\n",
      "iteration 95 - last reward: -64.09\n",
      "iteration 100 - last reward: -76.29\n",
      "iteration 105 - last reward: -7.73\n",
      "iteration 110 - last reward: -135.60\n",
      "iteration 115 - last reward: -21.64\n",
      "iteration 120 - last reward: -150.45\n",
      "iteration 125 - last reward: -70.00\n",
      "iteration 130 - last reward: -242.78\n",
      "iteration 135 - last reward: -265.27\n",
      "iteration 140 - last reward: -71.79\n",
      "iteration 145 - last reward: -102.36\n",
      "iteration 150 - last reward: -98.34\n",
      "iteration 155 - last reward: -101.03\n",
      "iteration 160 - last reward: -82.87\n",
      "iteration 165 - last reward: -59.56\n",
      "iteration 170 - last reward: -85.45\n",
      "iteration 175 - last reward: -12.32\n",
      "iteration 180 - last reward: 4.88\n",
      "iteration 185 - last reward: -36.39\n",
      "iteration 190 - last reward: -65.27\n",
      "iteration 195 - last reward: -85.86\n",
      "iteration 200 - last reward: 10.46\n",
      "iteration 205 - last reward: 89.00\n",
      "iteration 210 - last reward: -267.32\n",
      "iteration 215 - last reward: -90.78\n",
      "iteration 220 - last reward: 126.51\n",
      "iteration 225 - last reward: 4.62\n",
      "iteration 230 - last reward: 127.18\n",
      "iteration 235 - last reward: 65.95\n",
      "iteration 240 - last reward: 117.54\n",
      "iteration 245 - last reward: 212.82\n",
      "iteration 250 - last reward: -80.08\n",
      "iteration 255 - last reward: -71.20\n",
      "iteration 260 - last reward: 44.59\n",
      "iteration 265 - last reward: -280.97\n",
      "iteration 270 - last reward: 43.97\n",
      "iteration 275 - last reward: 190.93\n",
      "iteration 280 - last reward: 106.64\n",
      "iteration 285 - last reward: -229.79\n",
      "iteration 290 - last reward: -456.29\n",
      "iteration 295 - last reward: -51.66\n",
      "iteration 300 - last reward: 34.67\n",
      "iteration 305 - last reward: -7.33\n",
      "iteration 310 - last reward: -32.88\n",
      "iteration 315 - last reward: 237.99\n",
      "iteration 320 - last reward: 121.52\n",
      "iteration 325 - last reward: 160.20\n",
      "iteration 330 - last reward: -73.13\n",
      "iteration 335 - last reward: -77.48\n",
      "iteration 340 - last reward: 6.54\n",
      "iteration 345 - last reward: 215.47\n",
      "iteration 350 - last reward: 186.72\n",
      "iteration 355 - last reward: -4.25\n",
      "iteration 360 - last reward: -37.83\n",
      "iteration 365 - last reward: 177.40\n",
      "iteration 370 - last reward: 3.03\n",
      "iteration 375 - last reward: 224.81\n",
      "iteration 380 - last reward: 187.25\n",
      "iteration 385 - last reward: 5.59\n",
      "iteration 390 - last reward: 209.08\n",
      "iteration 395 - last reward: 179.43\n",
      "iteration 400 - last reward: -9.65\n",
      "iteration 405 - last reward: 223.84\n",
      "iteration 410 - last reward: 236.21\n",
      "iteration 415 - last reward: -10.65\n",
      "iteration 420 - last reward: 8.58\n",
      "iteration 425 - last reward: 266.58\n",
      "iteration 430 - last reward: 59.02\n",
      "iteration 435 - last reward: -15.89\n",
      "iteration 440 - last reward: 24.75\n",
      "iteration 445 - last reward: 250.83\n",
      "iteration 450 - last reward: 142.10\n",
      "iteration 455 - last reward: -201.75\n",
      "iteration 460 - last reward: 226.35\n",
      "iteration 465 - last reward: 165.34\n",
      "iteration 470 - last reward: 241.46\n",
      "iteration 475 - last reward: 243.72\n",
      "iteration 480 - last reward: 126.61\n",
      "iteration 485 - last reward: -17.30\n",
      "iteration 490 - last reward: 243.08\n",
      "iteration 495 - last reward: 111.92\n",
      "iteration 500 - last reward: 153.36\n",
      "iteration 505 - last reward: 208.22\n",
      "iteration 510 - last reward: 195.26\n",
      "iteration 515 - last reward: 230.44\n",
      "iteration 520 - last reward: 198.64\n",
      "iteration 525 - last reward: 251.79\n",
      "iteration 530 - last reward: -26.71\n",
      "iteration 535 - last reward: 263.52\n",
      "iteration 540 - last reward: 3.32\n",
      "iteration 545 - last reward: 4.71\n",
      "iteration 550 - last reward: -26.59\n",
      "iteration 555 - last reward: 14.62\n",
      "iteration 560 - last reward: 228.10\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Run deep Q-learning.\"\"\"\n",
    "\n",
    "# ---> TODO: find good hyperparameters\n",
    "discount_factor = 0.99   # because rewards come late (safe landings) and need long-term planning\n",
    "learning_rate = 0.001    # to avoid unstable Q-updates in a noisy, continuous state space\n",
    "epsilon_start = 1.0      # to explore all kinds of landings and failures at the beginning\n",
    "epsilon_end = 0.01       # to keep minimal exploration for edge cases after policy stabilizes\n",
    "epsilon_decay = 0.995    # to slowly shift from random moves to confident decisions over time\n",
    "\n",
    "# Create environment and reset it\n",
    "env = gym.make(\"LunarLander-v3\", continuous=False, render_mode=\"rgb_array\")\n",
    "observation, info = env.reset(seed=None)\n",
    "\n",
    "# Create Q-network and enable train mode\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "q_network = QNetwork().to(device)\n",
    "q_network.train()\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = optim.Adam(q_network.parameters(), lr=learning_rate)\n",
    "\n",
    "# Launch training\n",
    "running_reward = 0.0\n",
    "training_iteration = 0\n",
    "epsilon = epsilon_start\n",
    "while True:\n",
    "\n",
    "    # Reset the environment\n",
    "    observation, info = env.reset()\n",
    "    episode_total_reward = 0.0\n",
    "\n",
    "    # Sample a trajectory\n",
    "    while True:\n",
    "\n",
    "        # Epsilon-greedy action selection (random)\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        # Epsilon-greedy action selection (best action)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "\n",
    "                # Add batch dimension and transform to tensor\n",
    "                x = np.expand_dims(observation, 0)\n",
    "                x = torch.from_numpy(x).float().to(device)\n",
    "                q_values = q_network(x)\n",
    "\n",
    "                # ---> TODO: how to compute action\n",
    "                action = torch.argmax(q_values).item()\n",
    "\n",
    "        # Take the action\n",
    "        observation_next, reward, terminated, truncated, info = env.step(\n",
    "            action)\n",
    "\n",
    "        # Check if episode is done and save reward\n",
    "        done = terminated or truncated\n",
    "        episode_total_reward += reward\n",
    "\n",
    "        # Compute the TD target\n",
    "        with torch.no_grad():\n",
    "            x_next = np.expand_dims(observation_next, 0)\n",
    "            x_next = torch.from_numpy(x_next).float().to(device)\n",
    "            q_next = q_network(x_next)\n",
    "            q_next_max = q_next.max(dim=1).values.item()\n",
    "\n",
    "            # ---> TODO: compute the TD target\n",
    "            target = reward + discount_factor * q_next_max * (0 if done else 1)\n",
    "\n",
    "        # TD prediction\n",
    "        x = np.expand_dims(observation, 0)\n",
    "        x = torch.from_numpy(x).float().to(device)\n",
    "        q_pred = q_network(x)[0, action]\n",
    "\n",
    "        # ---> TODO: compute loss and update\n",
    "        loss = nn.functional.mse_loss(q_pred, torch.tensor(target, dtype = torch.float32).to(device))\n",
    "\n",
    "        # Reset gradients to 0.0\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Compute the gradients of the loss (backpropagation)\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the policy parameters (gradient ascent)\n",
    "        optimizer.step()\n",
    "\n",
    "        # Transition\n",
    "        observation = observation_next\n",
    "\n",
    "        # End episode\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Logging\n",
    "    running_reward = 0.1 * episode_total_reward + 0.9 * running_reward\n",
    "    epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
    "\n",
    "    # Log results\n",
    "    log_frequency = 5\n",
    "    training_iteration += 1\n",
    "    if training_iteration % log_frequency == 0:\n",
    "\n",
    "        # Save neural network\n",
    "        torch.save(q_network.state_dict(), \"q_network.pt\")\n",
    "\n",
    "        # Print results\n",
    "        print(\"iteration {} - last reward: {:.2f}\".format(\n",
    "            training_iteration, episode_total_reward))\n",
    "\n",
    "        # Exit condition\n",
    "        if running_reward >= 200:\n",
    "            break\n",
    "\n",
    "# Close environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "70460079-2711-4e1e-b9b6-3c6e5758fcc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_reward = 262.46410197990696\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Test Q-network.\"\"\"\n",
    "\n",
    "# Create environment and reset it\n",
    "env = gym.make(\"LunarLander-v3\", continuous=False, render_mode=\"human\")\n",
    "observation, info = env.reset(seed=None)\n",
    "total_reward = 0.0\n",
    "\n",
    "# Load trained Q-network and enable test mode\n",
    "device = torch.device(\"cpu\")\n",
    "q_network = QNetwork().to(device)\n",
    "q_network.load_state_dict(torch.load(\"q_network.pt\", weights_only=True))\n",
    "q_network.eval()\n",
    "\n",
    "# While the episode is not finished\n",
    "finished = False\n",
    "while not finished:\n",
    "\n",
    "    # Add batch dimension and transform to tensor\n",
    "    x = np.expand_dims(observation, 0)\n",
    "    x = torch.from_numpy(x).float().to(device)\n",
    "\n",
    "    # Compute action from the Q-table\n",
    "    q_values = q_network(x)\n",
    "\n",
    "    # ---> TODO: how to select an action\n",
    "    action = torch.argmax(q_values, dim=1).item()\n",
    "\n",
    "    # One step forward\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    finished = terminated or truncated\n",
    "\n",
    "    # Eventually render the environment (render mode should be \"human\")\n",
    "    total_reward += reward\n",
    "    env.render()\n",
    "\n",
    "# Print reward\n",
    "print(\"total_reward = {}\".format(total_reward))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806cd582-0fc6-45ce-b50f-d90121b5402d",
   "metadata": {},
   "source": [
    "## 3) REINFORCE algorithm\n",
    "\n",
    "We want to build a policy $\\pi_\\theta(a | s) = P(a | s, \\theta)$ that gives the probability of choosing an action $a$ in state $s$.\n",
    "The policy is a deep neural network parameterized by some weights $\\theta$.\n",
    "The policy is also referred to as \"actor\".\n",
    "\n",
    "We want to find the parameters $\\theta$ that maximize the performance measure $J(\\theta) = \\mathbb{E}_{\\pi_\\theta}[ G_0 ]$ with $G_t = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k+1}$ and $\\gamma \\in [0, 1]$ being a discount factor.\n",
    "To do so, we use the gradient ascent method: $\\theta_{k+1} = \\theta_{k} + \\alpha \\nabla_{\\theta_k} J(\\theta_k)$ with $\\alpha$ being the learning rate.\n",
    "The performance measure depends on both the action selection and the distribution of states.\n",
    "Both are affected by the policy parameters, which make the computation of the gradient challenging.\n",
    "\n",
    "The policy gradient theorem gives an expression for $\\nabla_\\theta J(\\theta)$ that does not involve the derivative of the state distribution.\n",
    "The expectation is over all possible state-action trajectories over the policy $\\pi_\\theta$:\n",
    "$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta}[ \\sum_{t=0}^{\\infty} G_t \\nabla_\\theta \\ln \\pi_\\theta(a_t | s_t) ]$.\n",
    "In the REINFORCE algorithm, we use a Monte-Carlo estimate over one episode, i.e., one trajectory:\n",
    "$\\nabla_\\theta J(\\theta) = \\sum_{t=0}^{\\infty} G_t \\nabla_\\theta \\ln \\pi_\\theta(a_t | s_t)$.\n",
    "\n",
    "Your objective is to complete the REINFORCE algorithm to train the policy until convergence. To solve the problem, you need to achieve a cumulative reward of at least 200 when training the policy. Below are: the code of the REINFORCE algorithm and a script to test your policy once it is trained.\n",
    "\n",
    "Below are 3 code samples.\n",
    "- A class implementing the policy. You must specify `input_size` and `nb_actions`.\n",
    "- A script to train the policy using REINFORCE. Complete the missing parts of the code.\n",
    "- A script to test the policy. Determine how to select an action from the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c4fd9d6-a246-4c3f-8a78-432f587e380c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorModel(nn.Module):\n",
    "    \"\"\"Deep neural network policy.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize.\"\"\"\n",
    "        super(ActorModel, self).__init__()\n",
    "        # --> TODO: specify the correct input and output sizes\n",
    "        input_size = 8\n",
    "        nb_actions = 4\n",
    "\n",
    "        # Layers\n",
    "        self.layer_a = nn.Linear(input_size, 128)\n",
    "        self.layer_b = nn.Linear(128, 128)\n",
    "        self.policy = nn.Linear(128, nb_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward.\"\"\"\n",
    "        x = F.relu(self.layer_a(x))\n",
    "        x = F.relu(self.layer_b(x))\n",
    "        action_prob = F.softmax(self.policy(x), dim=-1)\n",
    "        return action_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40d812eb-aa0d-4890-a34a-863967836e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 5 - last reward: -141.72\n",
      "iteration 10 - last reward: -188.17\n",
      "iteration 15 - last reward: -146.14\n",
      "iteration 20 - last reward: -117.40\n",
      "iteration 25 - last reward: -117.63\n",
      "iteration 30 - last reward: -19.22\n",
      "iteration 35 - last reward: -139.01\n",
      "iteration 40 - last reward: -187.32\n",
      "iteration 45 - last reward: -126.62\n",
      "iteration 50 - last reward: -192.32\n",
      "iteration 55 - last reward: -68.22\n",
      "iteration 60 - last reward: -85.34\n",
      "iteration 65 - last reward: -214.87\n",
      "iteration 70 - last reward: -145.13\n",
      "iteration 75 - last reward: -112.09\n",
      "iteration 80 - last reward: -56.32\n",
      "iteration 85 - last reward: -82.82\n",
      "iteration 90 - last reward: -109.25\n",
      "iteration 95 - last reward: -348.72\n",
      "iteration 100 - last reward: -184.89\n",
      "iteration 105 - last reward: -203.10\n",
      "iteration 110 - last reward: -316.28\n",
      "iteration 115 - last reward: -160.72\n",
      "iteration 120 - last reward: -395.96\n",
      "iteration 125 - last reward: -81.24\n",
      "iteration 130 - last reward: -63.47\n",
      "iteration 135 - last reward: -60.63\n",
      "iteration 140 - last reward: -389.08\n",
      "iteration 145 - last reward: -112.47\n",
      "iteration 150 - last reward: -230.23\n",
      "iteration 155 - last reward: -66.33\n",
      "iteration 160 - last reward: -58.38\n",
      "iteration 165 - last reward: -36.22\n",
      "iteration 170 - last reward: -91.62\n",
      "iteration 175 - last reward: -128.50\n",
      "iteration 180 - last reward: -60.06\n",
      "iteration 185 - last reward: -63.84\n",
      "iteration 190 - last reward: -39.43\n",
      "iteration 195 - last reward: -186.13\n",
      "iteration 200 - last reward: -18.67\n",
      "iteration 205 - last reward: -59.42\n",
      "iteration 210 - last reward: -128.01\n",
      "iteration 215 - last reward: -147.53\n",
      "iteration 220 - last reward: -75.22\n",
      "iteration 225 - last reward: -60.79\n",
      "iteration 230 - last reward: -106.80\n",
      "iteration 235 - last reward: -137.35\n",
      "iteration 240 - last reward: -172.42\n",
      "iteration 245 - last reward: -19.89\n",
      "iteration 250 - last reward: -10.40\n",
      "iteration 255 - last reward: -166.78\n",
      "iteration 260 - last reward: -69.27\n",
      "iteration 265 - last reward: -92.96\n",
      "iteration 270 - last reward: -265.98\n",
      "iteration 275 - last reward: -127.14\n",
      "iteration 280 - last reward: -122.54\n",
      "iteration 285 - last reward: -165.53\n",
      "iteration 290 - last reward: -53.84\n",
      "iteration 295 - last reward: -59.12\n",
      "iteration 300 - last reward: -67.68\n",
      "iteration 305 - last reward: -24.02\n",
      "iteration 310 - last reward: -71.15\n",
      "iteration 315 - last reward: -16.94\n",
      "iteration 320 - last reward: -206.89\n",
      "iteration 325 - last reward: -18.08\n",
      "iteration 330 - last reward: -4.06\n",
      "iteration 335 - last reward: -17.25\n",
      "iteration 340 - last reward: -9.23\n",
      "iteration 345 - last reward: -85.44\n",
      "iteration 350 - last reward: 9.15\n",
      "iteration 355 - last reward: -18.36\n",
      "iteration 360 - last reward: 6.28\n",
      "iteration 365 - last reward: -11.13\n",
      "iteration 370 - last reward: -16.07\n",
      "iteration 375 - last reward: 5.52\n",
      "iteration 380 - last reward: -44.11\n",
      "iteration 385 - last reward: 31.79\n",
      "iteration 390 - last reward: -27.30\n",
      "iteration 395 - last reward: -4.32\n",
      "iteration 400 - last reward: 2.80\n",
      "iteration 405 - last reward: -30.91\n",
      "iteration 410 - last reward: -37.65\n",
      "iteration 415 - last reward: -60.64\n",
      "iteration 420 - last reward: -70.83\n",
      "iteration 425 - last reward: -55.87\n",
      "iteration 430 - last reward: -7.93\n",
      "iteration 435 - last reward: -26.99\n",
      "iteration 440 - last reward: 26.00\n",
      "iteration 445 - last reward: -161.33\n",
      "iteration 450 - last reward: -225.86\n",
      "iteration 455 - last reward: -10.67\n",
      "iteration 460 - last reward: -21.17\n",
      "iteration 465 - last reward: -15.50\n",
      "iteration 470 - last reward: 50.63\n",
      "iteration 475 - last reward: 47.70\n",
      "iteration 480 - last reward: 37.56\n",
      "iteration 485 - last reward: 38.08\n",
      "iteration 490 - last reward: -47.15\n",
      "iteration 495 - last reward: 20.37\n",
      "iteration 500 - last reward: 8.05\n",
      "iteration 505 - last reward: -49.99\n",
      "iteration 510 - last reward: 43.31\n",
      "iteration 515 - last reward: -39.07\n",
      "iteration 520 - last reward: -113.85\n",
      "iteration 525 - last reward: 35.88\n",
      "iteration 530 - last reward: -32.05\n",
      "iteration 535 - last reward: -132.85\n",
      "iteration 540 - last reward: -23.45\n",
      "iteration 545 - last reward: -57.02\n",
      "iteration 550 - last reward: 19.26\n",
      "iteration 555 - last reward: -127.76\n",
      "iteration 560 - last reward: -166.86\n",
      "iteration 565 - last reward: -152.59\n",
      "iteration 570 - last reward: 38.33\n",
      "iteration 575 - last reward: 35.59\n",
      "iteration 580 - last reward: -43.38\n",
      "iteration 585 - last reward: 12.31\n",
      "iteration 590 - last reward: -8.69\n",
      "iteration 595 - last reward: 10.27\n",
      "iteration 600 - last reward: 36.03\n",
      "iteration 605 - last reward: -43.16\n",
      "iteration 610 - last reward: -16.69\n",
      "iteration 615 - last reward: 55.47\n",
      "iteration 620 - last reward: -48.86\n",
      "iteration 625 - last reward: -11.46\n",
      "iteration 630 - last reward: -221.63\n",
      "iteration 635 - last reward: 3.11\n",
      "iteration 640 - last reward: -131.98\n",
      "iteration 645 - last reward: -94.42\n",
      "iteration 650 - last reward: 126.43\n",
      "iteration 655 - last reward: -167.46\n",
      "iteration 660 - last reward: 122.49\n",
      "iteration 665 - last reward: -273.92\n",
      "iteration 670 - last reward: 45.11\n",
      "iteration 675 - last reward: 236.17\n",
      "iteration 680 - last reward: 44.19\n",
      "iteration 685 - last reward: 11.24\n",
      "iteration 690 - last reward: -47.92\n",
      "iteration 695 - last reward: -2.67\n",
      "iteration 700 - last reward: -37.89\n",
      "iteration 705 - last reward: 13.65\n",
      "iteration 710 - last reward: 61.74\n",
      "iteration 715 - last reward: 8.79\n",
      "iteration 720 - last reward: 73.54\n",
      "iteration 725 - last reward: 138.19\n",
      "iteration 730 - last reward: -29.54\n",
      "iteration 735 - last reward: 278.90\n",
      "iteration 740 - last reward: 194.31\n",
      "iteration 745 - last reward: 236.55\n",
      "iteration 750 - last reward: 185.39\n",
      "iteration 755 - last reward: 118.93\n",
      "iteration 760 - last reward: -50.92\n",
      "iteration 765 - last reward: -6.06\n",
      "iteration 770 - last reward: -48.26\n",
      "iteration 775 - last reward: 173.59\n",
      "iteration 780 - last reward: 239.69\n",
      "iteration 785 - last reward: 220.28\n",
      "iteration 790 - last reward: 60.01\n",
      "iteration 795 - last reward: 156.58\n",
      "iteration 800 - last reward: -112.84\n",
      "iteration 805 - last reward: -30.48\n",
      "iteration 810 - last reward: 56.41\n",
      "iteration 815 - last reward: 176.62\n",
      "iteration 820 - last reward: 0.75\n",
      "iteration 825 - last reward: -8.46\n",
      "iteration 830 - last reward: -9.74\n",
      "iteration 835 - last reward: 13.47\n",
      "iteration 840 - last reward: 23.52\n",
      "iteration 845 - last reward: 23.14\n",
      "iteration 850 - last reward: 210.69\n",
      "iteration 855 - last reward: -23.90\n",
      "iteration 860 - last reward: -55.19\n",
      "iteration 865 - last reward: 249.56\n",
      "iteration 870 - last reward: 46.19\n",
      "iteration 875 - last reward: 268.48\n",
      "iteration 880 - last reward: -10.60\n",
      "iteration 885 - last reward: 34.50\n",
      "iteration 890 - last reward: -94.66\n",
      "iteration 895 - last reward: 93.16\n",
      "iteration 900 - last reward: -97.70\n",
      "iteration 905 - last reward: 52.84\n",
      "iteration 910 - last reward: -9.33\n",
      "iteration 915 - last reward: -76.96\n",
      "iteration 920 - last reward: -102.62\n",
      "iteration 925 - last reward: -77.47\n",
      "iteration 930 - last reward: -117.90\n",
      "iteration 935 - last reward: -91.39\n",
      "iteration 940 - last reward: -91.83\n",
      "iteration 945 - last reward: -24.13\n",
      "iteration 950 - last reward: -22.90\n",
      "iteration 955 - last reward: 178.51\n",
      "iteration 960 - last reward: 224.39\n",
      "iteration 965 - last reward: 71.70\n",
      "iteration 970 - last reward: 208.94\n",
      "iteration 975 - last reward: 4.28\n",
      "iteration 980 - last reward: 25.53\n",
      "iteration 985 - last reward: 189.92\n",
      "iteration 990 - last reward: 252.24\n",
      "iteration 995 - last reward: 29.51\n",
      "iteration 1000 - last reward: 171.50\n",
      "iteration 1005 - last reward: 10.44\n",
      "iteration 1010 - last reward: 95.68\n",
      "iteration 1015 - last reward: 231.32\n",
      "iteration 1020 - last reward: 107.73\n",
      "iteration 1025 - last reward: -32.83\n",
      "iteration 1030 - last reward: -71.39\n",
      "iteration 1035 - last reward: 39.16\n",
      "iteration 1040 - last reward: -53.95\n",
      "iteration 1045 - last reward: 22.41\n",
      "iteration 1050 - last reward: 194.18\n",
      "iteration 1055 - last reward: 29.10\n",
      "iteration 1060 - last reward: 141.83\n",
      "iteration 1065 - last reward: 149.81\n",
      "iteration 1070 - last reward: 32.87\n",
      "iteration 1075 - last reward: 97.76\n",
      "iteration 1080 - last reward: 264.76\n",
      "iteration 1085 - last reward: 7.28\n",
      "iteration 1090 - last reward: 96.96\n",
      "iteration 1095 - last reward: 3.81\n",
      "iteration 1100 - last reward: 188.43\n",
      "iteration 1105 - last reward: -43.12\n",
      "iteration 1110 - last reward: 100.18\n",
      "iteration 1115 - last reward: -74.43\n",
      "iteration 1120 - last reward: -23.15\n",
      "iteration 1125 - last reward: 144.61\n",
      "iteration 1130 - last reward: 82.51\n",
      "iteration 1135 - last reward: 21.72\n",
      "iteration 1140 - last reward: 172.53\n",
      "iteration 1145 - last reward: 189.22\n",
      "iteration 1150 - last reward: 191.68\n",
      "iteration 1155 - last reward: 17.60\n",
      "iteration 1160 - last reward: -137.38\n",
      "iteration 1165 - last reward: -93.30\n",
      "iteration 1170 - last reward: 78.38\n",
      "iteration 1175 - last reward: 9.49\n",
      "iteration 1180 - last reward: -29.28\n",
      "iteration 1185 - last reward: 26.48\n",
      "iteration 1190 - last reward: 22.44\n",
      "iteration 1195 - last reward: -101.46\n",
      "iteration 1200 - last reward: -28.79\n",
      "iteration 1205 - last reward: -27.25\n",
      "iteration 1210 - last reward: 50.96\n",
      "iteration 1215 - last reward: 193.44\n",
      "iteration 1220 - last reward: 75.37\n",
      "iteration 1225 - last reward: 244.47\n",
      "iteration 1230 - last reward: 204.65\n",
      "iteration 1235 - last reward: 247.97\n",
      "iteration 1240 - last reward: 168.55\n",
      "iteration 1245 - last reward: 13.08\n",
      "iteration 1250 - last reward: -34.44\n",
      "iteration 1255 - last reward: 130.55\n",
      "iteration 1260 - last reward: 74.04\n",
      "iteration 1265 - last reward: 163.88\n",
      "iteration 1270 - last reward: 158.67\n",
      "iteration 1275 - last reward: 270.23\n",
      "iteration 1280 - last reward: 12.12\n",
      "iteration 1285 - last reward: 10.19\n",
      "iteration 1290 - last reward: 36.23\n",
      "iteration 1295 - last reward: 49.13\n",
      "iteration 1300 - last reward: 243.33\n",
      "iteration 1305 - last reward: 229.10\n",
      "iteration 1310 - last reward: 17.45\n",
      "iteration 1315 - last reward: 204.60\n",
      "iteration 1320 - last reward: 2.55\n",
      "iteration 1325 - last reward: -65.60\n",
      "iteration 1330 - last reward: 22.51\n",
      "iteration 1335 - last reward: -22.68\n",
      "iteration 1340 - last reward: 6.30\n",
      "iteration 1345 - last reward: 112.71\n",
      "iteration 1350 - last reward: 184.83\n",
      "iteration 1355 - last reward: -57.23\n",
      "iteration 1360 - last reward: -57.95\n",
      "iteration 1365 - last reward: 184.62\n",
      "iteration 1370 - last reward: 14.23\n",
      "iteration 1375 - last reward: 159.23\n",
      "iteration 1380 - last reward: 134.99\n",
      "iteration 1385 - last reward: 162.22\n",
      "iteration 1390 - last reward: 195.30\n",
      "iteration 1395 - last reward: -55.81\n",
      "iteration 1400 - last reward: -146.02\n",
      "iteration 1405 - last reward: -44.91\n",
      "iteration 1410 - last reward: -38.25\n",
      "iteration 1415 - last reward: -60.44\n",
      "iteration 1420 - last reward: -20.86\n",
      "iteration 1425 - last reward: 99.06\n",
      "iteration 1430 - last reward: 107.47\n",
      "iteration 1435 - last reward: 101.86\n",
      "iteration 1440 - last reward: 171.19\n",
      "iteration 1445 - last reward: 174.43\n",
      "iteration 1450 - last reward: 121.41\n",
      "iteration 1455 - last reward: 153.57\n",
      "iteration 1460 - last reward: 245.21\n",
      "iteration 1465 - last reward: 224.61\n",
      "iteration 1470 - last reward: 95.98\n",
      "iteration 1475 - last reward: 221.63\n",
      "iteration 1480 - last reward: 116.10\n",
      "iteration 1485 - last reward: 119.02\n",
      "iteration 1490 - last reward: 281.83\n",
      "iteration 1495 - last reward: 128.91\n",
      "iteration 1500 - last reward: 263.33\n",
      "iteration 1505 - last reward: 77.99\n",
      "iteration 1510 - last reward: 140.88\n",
      "iteration 1515 - last reward: 74.22\n",
      "iteration 1520 - last reward: 16.39\n",
      "iteration 1525 - last reward: 136.40\n",
      "iteration 1530 - last reward: 243.42\n",
      "iteration 1535 - last reward: 49.52\n",
      "iteration 1540 - last reward: 127.80\n",
      "iteration 1545 - last reward: 279.83\n",
      "iteration 1550 - last reward: 197.21\n",
      "iteration 1555 - last reward: 139.28\n",
      "iteration 1560 - last reward: 114.50\n",
      "iteration 1565 - last reward: 98.93\n",
      "iteration 1570 - last reward: 247.17\n",
      "iteration 1575 - last reward: 21.81\n",
      "iteration 1580 - last reward: 180.65\n",
      "iteration 1585 - last reward: 66.13\n",
      "iteration 1590 - last reward: 91.71\n",
      "iteration 1595 - last reward: 88.59\n",
      "iteration 1600 - last reward: 13.37\n",
      "iteration 1605 - last reward: 222.35\n",
      "iteration 1610 - last reward: 39.62\n",
      "iteration 1615 - last reward: 50.18\n",
      "iteration 1620 - last reward: 68.82\n",
      "iteration 1625 - last reward: 248.41\n",
      "iteration 1630 - last reward: 231.87\n",
      "iteration 1635 - last reward: 234.59\n",
      "iteration 1640 - last reward: 193.90\n",
      "iteration 1645 - last reward: 196.40\n",
      "iteration 1650 - last reward: 212.46\n",
      "iteration 1655 - last reward: 51.40\n",
      "iteration 1660 - last reward: 26.70\n",
      "iteration 1665 - last reward: -16.29\n",
      "iteration 1670 - last reward: 29.62\n",
      "iteration 1675 - last reward: 99.35\n",
      "iteration 1680 - last reward: -36.97\n",
      "iteration 1685 - last reward: -4.24\n",
      "iteration 1690 - last reward: -38.01\n",
      "iteration 1695 - last reward: 157.73\n",
      "iteration 1700 - last reward: -28.91\n",
      "iteration 1705 - last reward: -48.81\n",
      "iteration 1710 - last reward: -36.29\n",
      "iteration 1715 - last reward: 149.27\n",
      "iteration 1720 - last reward: -128.52\n",
      "iteration 1725 - last reward: -61.94\n",
      "iteration 1730 - last reward: -61.00\n",
      "iteration 1735 - last reward: -106.79\n",
      "iteration 1740 - last reward: -49.19\n",
      "iteration 1745 - last reward: -5.96\n",
      "iteration 1750 - last reward: -76.28\n",
      "iteration 1755 - last reward: 102.34\n",
      "iteration 1760 - last reward: 50.53\n",
      "iteration 1765 - last reward: 170.08\n",
      "iteration 1770 - last reward: 21.37\n",
      "iteration 1775 - last reward: -13.89\n",
      "iteration 1780 - last reward: 139.91\n",
      "iteration 1785 - last reward: 103.42\n",
      "iteration 1790 - last reward: 153.27\n",
      "iteration 1795 - last reward: 69.26\n",
      "iteration 1800 - last reward: 44.94\n",
      "iteration 1805 - last reward: 66.54\n",
      "iteration 1810 - last reward: 42.77\n",
      "iteration 1815 - last reward: 220.27\n",
      "iteration 1820 - last reward: 213.03\n",
      "iteration 1825 - last reward: 101.72\n",
      "iteration 1830 - last reward: 247.71\n",
      "iteration 1835 - last reward: 201.06\n",
      "iteration 1840 - last reward: 269.78\n",
      "iteration 1845 - last reward: 232.87\n",
      "iteration 1850 - last reward: 160.46\n",
      "iteration 1855 - last reward: 99.07\n",
      "iteration 1860 - last reward: 121.06\n",
      "iteration 1865 - last reward: 69.87\n",
      "iteration 1870 - last reward: 111.27\n",
      "iteration 1875 - last reward: 146.71\n",
      "iteration 1880 - last reward: 137.41\n",
      "iteration 1885 - last reward: -9.39\n",
      "iteration 1890 - last reward: 243.69\n",
      "iteration 1895 - last reward: 31.64\n",
      "iteration 1900 - last reward: 275.97\n",
      "iteration 1905 - last reward: 2.84\n",
      "iteration 1910 - last reward: 44.88\n",
      "iteration 1915 - last reward: -9.64\n",
      "iteration 1920 - last reward: 137.09\n",
      "iteration 1925 - last reward: 177.90\n",
      "iteration 1930 - last reward: 159.17\n",
      "iteration 1935 - last reward: 41.87\n",
      "iteration 1940 - last reward: 110.35\n",
      "iteration 1945 - last reward: 188.61\n",
      "iteration 1950 - last reward: 133.85\n",
      "iteration 1955 - last reward: 210.97\n",
      "iteration 1960 - last reward: 119.87\n",
      "iteration 1965 - last reward: 204.99\n",
      "iteration 1970 - last reward: 143.89\n",
      "iteration 1975 - last reward: 16.96\n",
      "iteration 1980 - last reward: 85.72\n",
      "iteration 1985 - last reward: -24.53\n",
      "iteration 1990 - last reward: 23.84\n",
      "iteration 1995 - last reward: -35.76\n",
      "iteration 2000 - last reward: 2.26\n",
      "iteration 2005 - last reward: 115.63\n",
      "iteration 2010 - last reward: -15.35\n",
      "iteration 2015 - last reward: 195.66\n",
      "iteration 2020 - last reward: 67.18\n",
      "iteration 2025 - last reward: 126.60\n",
      "iteration 2030 - last reward: 132.82\n",
      "iteration 2035 - last reward: 126.42\n",
      "iteration 2040 - last reward: 117.97\n",
      "iteration 2045 - last reward: 153.19\n",
      "iteration 2050 - last reward: 138.75\n",
      "iteration 2055 - last reward: 141.02\n",
      "iteration 2060 - last reward: 130.50\n",
      "iteration 2065 - last reward: 139.92\n",
      "iteration 2070 - last reward: 140.02\n",
      "iteration 2075 - last reward: 159.70\n",
      "iteration 2080 - last reward: 128.03\n",
      "iteration 2085 - last reward: 115.78\n",
      "iteration 2090 - last reward: 175.12\n",
      "iteration 2095 - last reward: 12.25\n",
      "iteration 2100 - last reward: 110.96\n",
      "iteration 2105 - last reward: 181.68\n",
      "iteration 2110 - last reward: 125.41\n",
      "iteration 2115 - last reward: 147.83\n",
      "iteration 2120 - last reward: 19.33\n",
      "iteration 2125 - last reward: 135.12\n",
      "iteration 2130 - last reward: 109.20\n",
      "iteration 2135 - last reward: 175.48\n",
      "iteration 2140 - last reward: 62.02\n",
      "iteration 2145 - last reward: 158.51\n",
      "iteration 2150 - last reward: 132.04\n",
      "iteration 2155 - last reward: 125.62\n",
      "iteration 2160 - last reward: 265.53\n",
      "iteration 2165 - last reward: 212.18\n",
      "iteration 2170 - last reward: 253.77\n",
      "iteration 2175 - last reward: 88.71\n",
      "iteration 2180 - last reward: 102.60\n",
      "iteration 2185 - last reward: 204.14\n",
      "iteration 2190 - last reward: 205.12\n",
      "iteration 2195 - last reward: 62.51\n",
      "iteration 2200 - last reward: 187.55\n",
      "iteration 2205 - last reward: 193.58\n",
      "iteration 2210 - last reward: 189.13\n",
      "iteration 2215 - last reward: 56.07\n",
      "iteration 2220 - last reward: 64.81\n",
      "iteration 2225 - last reward: 127.50\n",
      "iteration 2230 - last reward: 50.55\n",
      "iteration 2235 - last reward: 178.36\n",
      "iteration 2240 - last reward: 214.73\n",
      "iteration 2245 - last reward: 227.39\n",
      "iteration 2250 - last reward: 168.97\n",
      "iteration 2255 - last reward: 89.84\n",
      "iteration 2260 - last reward: 198.73\n",
      "iteration 2265 - last reward: 235.55\n",
      "iteration 2270 - last reward: 239.00\n",
      "iteration 2275 - last reward: 213.73\n",
      "iteration 2280 - last reward: 140.45\n",
      "iteration 2285 - last reward: 271.16\n",
      "iteration 2290 - last reward: 138.57\n",
      "iteration 2295 - last reward: 122.57\n",
      "iteration 2300 - last reward: 137.10\n",
      "iteration 2305 - last reward: 63.45\n",
      "iteration 2310 - last reward: 137.67\n",
      "iteration 2315 - last reward: 101.77\n",
      "iteration 2320 - last reward: 234.58\n",
      "iteration 2325 - last reward: 110.51\n",
      "iteration 2330 - last reward: 264.03\n",
      "iteration 2335 - last reward: 205.82\n",
      "iteration 2340 - last reward: 207.91\n",
      "iteration 2345 - last reward: 229.68\n",
      "iteration 2350 - last reward: 253.97\n",
      "iteration 2355 - last reward: 240.28\n",
      "iteration 2360 - last reward: 129.85\n",
      "iteration 2365 - last reward: 280.38\n",
      "iteration 2370 - last reward: 113.31\n",
      "iteration 2375 - last reward: 98.93\n",
      "iteration 2380 - last reward: 268.72\n",
      "iteration 2385 - last reward: 155.72\n",
      "iteration 2390 - last reward: 144.38\n",
      "iteration 2395 - last reward: 116.44\n",
      "iteration 2400 - last reward: 104.13\n",
      "iteration 2405 - last reward: 100.77\n",
      "iteration 2410 - last reward: 127.75\n",
      "iteration 2415 - last reward: 146.55\n",
      "iteration 2420 - last reward: 138.77\n",
      "iteration 2425 - last reward: 100.76\n",
      "iteration 2430 - last reward: 115.46\n",
      "iteration 2435 - last reward: 168.95\n",
      "iteration 2440 - last reward: 143.07\n",
      "iteration 2445 - last reward: 137.73\n",
      "iteration 2450 - last reward: 119.35\n",
      "iteration 2455 - last reward: 35.36\n",
      "iteration 2460 - last reward: 79.57\n",
      "iteration 2465 - last reward: 109.00\n",
      "iteration 2470 - last reward: 190.16\n",
      "iteration 2475 - last reward: 256.29\n",
      "iteration 2480 - last reward: 81.59\n",
      "iteration 2485 - last reward: 134.67\n",
      "iteration 2490 - last reward: 203.17\n",
      "iteration 2495 - last reward: 134.30\n",
      "iteration 2500 - last reward: 204.92\n",
      "iteration 2505 - last reward: 204.89\n",
      "iteration 2510 - last reward: 231.40\n",
      "iteration 2515 - last reward: 234.80\n",
      "iteration 2520 - last reward: 121.37\n",
      "iteration 2525 - last reward: 118.18\n",
      "iteration 2530 - last reward: 137.19\n",
      "iteration 2535 - last reward: 144.74\n",
      "iteration 2540 - last reward: 120.42\n",
      "iteration 2545 - last reward: 142.41\n",
      "iteration 2550 - last reward: 112.53\n",
      "iteration 2555 - last reward: 118.64\n",
      "iteration 2560 - last reward: 184.03\n",
      "iteration 2565 - last reward: 109.59\n",
      "iteration 2570 - last reward: 93.39\n",
      "iteration 2575 - last reward: 143.95\n",
      "iteration 2580 - last reward: 276.79\n",
      "iteration 2585 - last reward: 244.79\n",
      "iteration 2590 - last reward: 224.21\n",
      "iteration 2595 - last reward: 123.16\n",
      "iteration 2600 - last reward: -8.12\n",
      "iteration 2605 - last reward: 143.87\n",
      "iteration 2610 - last reward: 265.51\n",
      "iteration 2615 - last reward: 38.87\n",
      "iteration 2620 - last reward: 15.19\n",
      "iteration 2625 - last reward: 0.23\n",
      "iteration 2630 - last reward: 48.59\n",
      "iteration 2635 - last reward: 47.04\n",
      "iteration 2640 - last reward: -2.61\n",
      "iteration 2645 - last reward: 269.64\n",
      "iteration 2650 - last reward: 40.72\n",
      "iteration 2655 - last reward: 97.85\n",
      "iteration 2660 - last reward: 67.80\n",
      "iteration 2665 - last reward: 155.78\n",
      "iteration 2670 - last reward: 225.66\n",
      "iteration 2675 - last reward: 293.72\n",
      "iteration 2680 - last reward: 256.87\n",
      "iteration 2685 - last reward: 147.73\n",
      "iteration 2690 - last reward: 265.16\n",
      "iteration 2695 - last reward: 144.98\n",
      "iteration 2700 - last reward: 219.05\n",
      "iteration 2705 - last reward: 247.04\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Run REINFORCE.\"\"\"\n",
    "\n",
    "# ---> TODO: find good hyperparameters\n",
    "discount_factor = 0.99  # encourages long-term reward, fits LunarLander  \n",
    "learning_rate = 0.001   # to avoid unstable updates from high-variance policy gradients in REINFORCE\n",
    "\n",
    "# Create environment and reset it\n",
    "env = gym.make(\"LunarLander-v3\", continuous=False, render_mode=\"rgb_array\")\n",
    "obsevation, info = env.reset(seed=None)\n",
    "\n",
    "# Create policy and enable train mode\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "policy = ActorModel().to(device)\n",
    "policy.train()\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = optim.Adam(policy.parameters(), lr=learning_rate)\n",
    "\n",
    "# Launch training\n",
    "running_reward = 0.0\n",
    "training_iteration = 0\n",
    "while True:\n",
    "\n",
    "    # Experience\n",
    "    # ------------------------------------------\n",
    "\n",
    "    # Reset the environment\n",
    "    obsevation, info = env.reset()\n",
    "\n",
    "    # During experience, we will save:\n",
    "    # - the probability of the chosen action at each time step pi(at|st)\n",
    "    # - the rewards received at each time step ri\n",
    "    saved_probabilities = list()\n",
    "    saved_rewards = list()\n",
    "\n",
    "    # Sample a trajectory\n",
    "    while True:\n",
    "\n",
    "        # Add batch dimension and transform to tensor\n",
    "        x = torch.from_numpy(np.expand_dims(obsevation, 0)).float()\n",
    "\n",
    "        # Create a categorical distribution over the list of probabilities\n",
    "        # of actions (given by the policy) and sample an action from it\n",
    "        probabilities = policy(x.to(device))\n",
    "        distribution = Categorical(probabilities)\n",
    "        action = distribution.sample()\n",
    "\n",
    "        # Take the action\n",
    "        obsevation, reward, terminated, truncated, info = env.step(\n",
    "            action.item())\n",
    "\n",
    "        # Save the probability of the chosen action and the reward\n",
    "        saved_probabilities.append(probabilities[0][action])\n",
    "        saved_rewards.append(reward)\n",
    "\n",
    "        # End episode\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    # Compute discounted sum of rewards\n",
    "\n",
    "\n",
    "    # ------------------------------------------\n",
    "\n",
    "    # Current discounted reward\n",
    "    discounted_reward = 0.0\n",
    "\n",
    "    # List of all the discounted rewards, for each time step\n",
    "    discounted_rewards = list()\n",
    "\n",
    "    # ---> TODO: compute discounted rewards\n",
    "    for r in saved_rewards[::-1]:\n",
    "        discounted_reward = r + discount_factor * discounted_reward\n",
    "        discounted_rewards.insert(0, discounted_reward)\n",
    "\n",
    "    # Eventually normalize for stability purposes\n",
    "    discounted_rewards = torch.tensor(discounted_rewards)\n",
    "    mean, std = discounted_rewards.mean(), discounted_rewards.std()\n",
    "    discounted_rewards = (discounted_rewards - mean) / (std + 1e-7)\n",
    "\n",
    "    # Update policy parameters\n",
    "    # ------------------------------------------\n",
    "\n",
    "    # For each time step\n",
    "    actor_loss = list()\n",
    "    for p, g in zip(saved_probabilities, discounted_rewards):\n",
    "\n",
    "        # ---> TODO: compute policy loss\n",
    "        time_step_actor_loss = -torch.log(p) * g\n",
    "\n",
    "        # Save it\n",
    "        actor_loss.append(time_step_actor_loss.view(1))\n",
    "\n",
    "    # Sum all the time step losses\n",
    "    actor_loss = torch.cat(actor_loss).sum()\n",
    "\n",
    "    # Reset gradients to 0.0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Compute the gradients of the loss (backpropagation)\n",
    "    actor_loss.backward()\n",
    "\n",
    "    # Update the policy parameters (gradient ascent)\n",
    "    optimizer.step()\n",
    "\n",
    "    # Logging\n",
    "    # ------------------------------------------\n",
    "\n",
    "    # Episode total reward\n",
    "    episode_total_reward = sum(saved_rewards)\n",
    "    running_reward = 0.1 * episode_total_reward + 0.9 * running_reward\n",
    "\n",
    "    # Log results\n",
    "    log_frequency = 5\n",
    "    training_iteration += 1\n",
    "    if training_iteration % log_frequency == 0:\n",
    "\n",
    "        # Save neural network\n",
    "        torch.save(policy.state_dict(), \"policy.pt\")\n",
    "\n",
    "        # Print results\n",
    "        print(\"iteration {} - last reward: {:.2f}\".format(\n",
    "            training_iteration, episode_total_reward))\n",
    "\n",
    "        # Exit condition\n",
    "        if running_reward >= 200:\n",
    "            break\n",
    "\n",
    "# Close environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffcbf7d4-88b1-44cd-b61c-7438cb2a5908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_reward = 105.0579070455011\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Test policy.\"\"\"\n",
    "# Create environment and reset it\n",
    "env = gym.make(\"LunarLander-v3\", continuous=False, render_mode=\"human\")\n",
    "obsevation, info = env.reset(seed=None)\n",
    "total_reward = 0.0\n",
    "\n",
    "# Load trained policy and enable test mode\n",
    "device = torch.device(\"cpu\")\n",
    "policy = ActorModel().to(device)\n",
    "policy.load_state_dict(torch.load(\"policy.pt\", weights_only=True))\n",
    "policy.eval()\n",
    "\n",
    "# While the episode is not finished\n",
    "finished = False\n",
    "while not finished:\n",
    "\n",
    "    # Add batch dimension and transform to tensor\n",
    "    x = torch.from_numpy(np.expand_dims(obsevation, 0)).float()\n",
    "\n",
    "    # Compute action from the policy\n",
    "    action = policy(x.to(device))\n",
    "\n",
    "    # ---> TODO: how to select an action\n",
    "    with torch.no_grad():\n",
    "        x = torch.from_numpy(np.expand_dims(obsevation, 0)).float().to(device)\n",
    "        probabilities = policy(x)\n",
    "\n",
    "        # Select action with highest probability\n",
    "        action = torch.argmax(probabilities, dim=1).item()\n",
    "\n",
    "    # One step forward\n",
    "    obsevation, reward, terminated, truncated, info = env.step(action)\n",
    "    finished = terminated or truncated\n",
    "\n",
    "    # Eventually render the environment (render mode should be \"human\")\n",
    "    total_reward += reward\n",
    "    env.render()\n",
    "\n",
    "# Print reward\n",
    "print(\"total_reward = {}\".format(total_reward))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9717c5-a8dd-4e87-b32c-ecb3f0c4c274",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
